{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3c0b710-cf7a-4f0f-a604-5cf2d2e58b4a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<hr style=\"height:0px; visibility:hidden;\" />\n",
    "\n",
    "<h1><center>5. Amplicon processing</center></h1>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Now we're ready to start getting into the actual processing! Note that this is R kernel we are working in now, and while we will break down most of the code we see, don't feel like you need to digest and completely understand all of the R code right away.\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<center>This is notebook 5 of 6 of <a href=\"00-overview.ipynb\">GL4U's Amplicon Bootcamp</a>. It is expected that the previous notebooks have been completed already.</center>\n",
    "\n",
    "---\n",
    "\n",
    "[**Previous:** 4. Setup and QC](04-setup-QC.ipynb)\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: right\"><a href=\"06-amplicon-analysis.ipynb\"><b>Next:</b> 6. Amplicon analysis</a></div>\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50004340-9df6-4ea1-9065-1f803896d7dd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "## Setting up our environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05785141-3b7b-43d7-8845-1f3a0a014bcd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "### Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b44f19-afb8-4832-8308-447b1e976298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "library(dada2)\n",
    "library(tidyverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839a6a1f-d0bc-4ca2-85c4-fe4cba6331e7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "### Setting our location and some general variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8589154f-ec38-4365-818e-e26211abc36a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "setwd(\"~/GL4U-amplicon-tutorial/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83640ff-cbc6-4202-a464-46f103c27a02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list.files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb8094c-cf10-41f2-96f9-766262506ba6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_reads_dir <- \"raw-reads\"\n",
    "trimmed_and_filtered_reads_dir <- \"trimmed-and-filtered-reads\"\n",
    "fastqc_outputs_dir <- \"fastqc-outputs\"\n",
    "final_outputs_dir <- \"final-outputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb4a17b-7e75-49d9-90c6-41b0b46a21e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reading in our sample info table\n",
    "sample_info_tab <- read.table(file = \"sample-info.tsv\", header = TRUE, sep = \"\\t\", row.names = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9367b12-a814-4ad2-b942-3a87332eca74",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Code Breakdown</b>\n",
    "<br>\n",
    "\n",
    "- `read.table()`      - the primary function we're using\n",
    "    - `file = `       - where we specify the input file we want to read\n",
    "    - `header = `     - where we state if the first row should be treated as a header (TRUE/FALSE)\n",
    "    - `sep = `        - where we specify the delimiter that separates values (\"\\t\" is for tab)\n",
    "    - `row.names = `  - where we can tell it if any columns should be treated as row names, this says to use the first column (would set to `NULL` if we wanted to explicitly not use any column as row names)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61de9d3f-78f9-4c0e-9c91-3479bf94ef86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_info_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d56fbc-bd82-4657-a900-dfd5b8cae5b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_names <- row.names(sample_info_tab)\n",
    "sample_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2bcae6-9822-4223-a33d-2f7a395403e4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "### Creating some variables to help with processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3495f8-ec20-4a88-a796-b68160e3ade4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# making an object that holds all forward read starting files\n",
    "forward_raw_files <- list.files(path = raw_reads_dir, pattern = \"*R1_raw.fastq.gz\", full.names = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a26933c-9810-48e7-a6be-e206c9d5dd47",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Code Breakdown</b>\n",
    "<br>\n",
    "\n",
    "- `list.files()`      - the primary function we're using\n",
    "    - `path = `       - the location to look for files\n",
    "    - `pattern = `    - the pattern to match, here we are using the `*` wildcard like before, to say anything that ends with \"R1_raw.fastq.gz\"\n",
    "    - `full.names = ` - here allows us to specify if we want to retain the directory structure leading to the files, which we do want in this case (TRUE/FALSE)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0888c94-661d-4d34-978a-94f92088cc2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# looking at them\n",
    "forward_raw_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8db3f3f-bb85-45a4-904a-944b8b1150e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now doing the same for the reverse reads\n",
    "reverse_raw_files <- list.files(raw_reads_dir, pattern = \"*R2_raw.fastq.gz\", full.names = TRUE)\n",
    "reverse_raw_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec4e349-cca4-4315-83a2-dbd64785b924",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# making an object holding what will be the output trimmed/filtered forward files\n",
    "forward_filtered_files <- paste0(trimmed_and_filtered_reads_dir, \"/\", sample_names, \"_R1_filtered.fastq.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae615ad7-2696-4877-9b62-ebcbaf57ae7c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Code Breakdown</b>\n",
    "<br>\n",
    "\n",
    "- `paste0()`      - the primary function we're using\n",
    "    - `... `      - all arguments we give as just positional arugments like the above will just be stuck together\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7176a1bf-7bd7-48f1-86d6-1658c345a3a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forward_filtered_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9073bb63-54e2-4e5d-ab46-8090f8f80a75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now doing the same for the reverse reads\n",
    "reverse_filtered_files <- paste0(trimmed_and_filtered_reads_dir, \"/\", sample_names, \"_R2_filtered.fastq.gz\")\n",
    "reverse_filtered_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6293c333-44b5-4eac-a1ea-9454cdd1f8ac",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8303be3-bcf9-4282-9c22-8fe1e347c89a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "## Checking on settings to remove primers\n",
    "\n",
    "It is imperative that we properly remove the primers otherwise we will end up with non-biological sequences introduced due to the amibugous bases in the primers that were used. We can try trimming the primers with dada2's quality trimming/filtering program. But before we run it on everything, we're going to closely look at and test things on one sample – looking at the sequences before and after so we can visibly check the primers are indeed being removed.\n",
    "\n",
    "These are the primers for this dataset, and the IUPAC degenerate-base codes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee910c10-0244-406d-a8ce-8ea30afd9aef",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "```\n",
    "f primer: GTGYCAGCMGCCGCGGTAA\n",
    "r primer: GGACTACNVGGGTWTCTAAT\n",
    "\n",
    "Y = C/T  \n",
    "M = A/C  \n",
    "N = A/T/G/C  \n",
    "V = A/C/G  \n",
    "W = A/T  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f6a899-feb0-4715-a8a4-8332932fe806",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "### Ensuring we can spot the primers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a6dfb2-97ec-455e-ac85-537ebabecab6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# establishing a connection with the F10 forward read file\n",
    "fwd_test_file <- paste0(raw_reads_dir, \"/F10_R1_raw.fastq.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6887093d-a166-4f55-905a-549377c43307",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fwd_test_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a56f94f-35b4-4e60-8fc3-ca9c8652d3b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "incon <- gzcon(file(fwd_test_file, open = \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425cbc45-7f8c-43b5-a0a5-9591e29a334c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this reads in the first 8 lines, with each set of 4 lines holding one fastq entry\n",
    "fwd_lines <- readLines(incon, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bd09ed-79fe-4421-bc81-cb16a6b4fa6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# here is how we can just get the sequences for the first 2 entries\n",
    "fwd_lines[c(2,6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dde8282-d274-4897-8db7-3a5a45121b0a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "They each start exactly with the forward primer sequence right up front, which isn't always the case (the asterisks are over the degenerate bases):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457e12c2-526f-408e-b23a-c8afd5078676",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "```\n",
    "                           *    *\n",
    "forward primer:         GTGYCAGCMGCCGCGGTAA\n",
    "forward read 1 start:   GTGCCAGCAGCCGCGGTAA\n",
    "forward read 2 start:   GTGCCAGCCGCCGCGGTAA\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a435f14a-39ed-4ab3-947d-bfa53fb57a82",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Let's look at a couple reverse reads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde5a55d-8a52-4c37-bce7-7d800c7f7717",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# establishing a connect with the F10 forward read file\n",
    "rev_test_file <- paste0(raw_reads_dir, \"/F10_R2_raw.fastq.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47496e96-3d2e-4419-9d85-86e9b2ba3f04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rev_test_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176ff446-0112-4926-bd9a-7bd74118849b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# establishing a connection\n",
    "incon <- gzcon(file(rev_test_file, open = \"rb\"))\n",
    "\n",
    "# storing the first 8 lines in a variable\n",
    "rev_lines <- readLines(incon, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcecaa9-de85-44fe-8c81-f86b982d4b28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# and looking at the first 2 sequences\n",
    "rev_lines[c(2,6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d962a98-6ab1-4694-b2a5-5d9c0e812f5f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "They each start exactly with the reverse primer sequence right up front:\n",
    "\n",
    "```\n",
    "                                **    *\n",
    "reverse primer:          GGACTACNVGGGTWTCTAAT\n",
    "reverse read 1 start:    GGACTACTAGGGTTTCTAAT\n",
    "reverse read 2 start:    GGACTACCCGGGTTTCTAAT\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462670d0-4d8d-45dc-b89a-80fd758423af",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "### Doing a test trimming where we specify to cut these off\n",
    "The forward primer is 19 bases, the reverse is 20. We can pass these values to the `trimLeft` argument of dada2's `filterAndTrim()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a987295-1b46-4081-8b45-412cdfafcfd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filterAndTrim(fwd = fwd_test_file, \n",
    "              rev = rev_test_file, \n",
    "              filt = \"test-F10_R1_filtered.fastq.gz\",\n",
    "              filt.rev = \"test-F10_R2_filtered.fastq.gz\", \n",
    "              trimLeft = c(19, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fec030-3bb6-424e-a78e-9f6d90f0b2bd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Code Breakdown</b>\n",
    "<br>\n",
    "\n",
    "- `filterAndTrim()` - primary function\n",
    "    - `fwd = `      - where we provide the object holding all the forward read input file(s)\n",
    "    - `rev = `      - where we provide the object holding all the reverse read input file(s)\n",
    "    - `filt = `     - where we provide the object holding what will be the output forward read file(s)\n",
    "    - `filt.rev = ` - where we provide the object holding what will be the output reverse read file(s)\n",
    "    - `trimLeft = ` - how many bases we want to have trimmed off the left side of the reads (providing them as a vector like done above with two numbers means the first will be used for the forward reads and the second for the reverse reads)\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e907f09a-0e41-4836-b54a-baccd4e751ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list.files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af612a3-19cf-4fc6-8c22-cb32decb45a9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "### Ensuring those settings successfully removed the primers\n",
    "Now we are going to peek at the output trimmed files to make sure we cut off the primers, doing the same things we did above to read in part of the file and then just look at the first 2 sequences of the forward and reverse reads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbba7b23-0893-4f6d-b8ca-c6a8bf2b58b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# establishing a connection and storing the first 8 lines into a file in one line now\n",
    "fwd_filt_lines <- gzcon(file(\"test-F10_R1_filtered.fastq.gz\", open = \"rb\")) %>% readLines(8)\n",
    "    # reminder that this is the same as writing things nested this way\n",
    "# fwd_filt_lines <- readLines(gzcon(file(\"F10_R1_filtered.fastq.gz\", open = \"rb\")), 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2445cc-3879-479a-abb3-7576b0b62c46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fwd_filt_lines[c(2,6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48235ec2-2575-4e88-a3ac-0ae6158511a0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "These previously started:\n",
    "\n",
    "```\n",
    "                            *    *\n",
    "forward primer:          GTGYCAGCMGCCGCGGTAA\n",
    "original fwd read 1:     GTGCCAGCAGCCGCGGTAA   TACGGAGGAT\n",
    "original fwd read 2:     GTGCCAGCCGCCGCGGTAA   TACGTAGGGG\n",
    "```\n",
    "\n",
    "They each now begin right after the forward primer 👍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ef06ca-afe1-42ff-bf9e-90c61757886e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rev_filt_lines <- gzcon(file(\"test-F10_R2_filtered.fastq.gz\", open = \"rb\")) %>% readLines(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2473c24-61d3-4ad7-9023-fddd2047adcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rev_filt_lines[c(2,6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87000b13-d391-47be-81c7-ca5ccf18e77f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "These previously started:\n",
    "\n",
    "```\n",
    "                                **    *\n",
    "reverse primer:          GGACTACNVGGGTWTCTAAT\n",
    "original rev read 1:     GGACTACTAGGGTTTCTAAT  CCTGTTTGAT\n",
    "original rev read 2:     GGACTACCCGGGTTTCTAAT  CCTTTTTGCT\n",
    "```\n",
    "\n",
    "They each now begin right after the reverse primer 👍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5587d6ad-8ccc-4089-9d40-401406690325",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "So with that confirmation (or looking at some more samples if wanted), we can be fairly confident in using that `trimLeft` argument for all our samples to remove the primers (since these were all prepared and sequenced together the same way).\n",
    "\n",
    "Now just removing those test output files so we know for sure we run everything the same way when we do all of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5e2583-17a6-4eda-81c3-1ec7718b2276",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file.remove(\"test-F10_R1_filtered.fastq.gz\", \"test-F10_R2_filtered.fastq.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81419487-31da-448a-a69d-33ce934e986d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list.files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462e371c-f9d1-4043-8111-076feaa2b4dd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484ebcd-0afb-4985-85c2-5016a2526446",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "## Processing with dada2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e4be0e-6b57-429a-9472-692b60bd746f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "### Quality trimming/filtering (including removing primers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f5c2fa-ccb6-49fd-8c80-0b3b5241d19b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_out <- filterAndTrim(fwd = forward_raw_files, \n",
    "                              rev = reverse_raw_files, \n",
    "                              filt = forward_filtered_files, \n",
    "                              filt.rev = reverse_filtered_files, \n",
    "                              trimLeft = c(19, 20), \n",
    "                              maxEE = c(1,1),\n",
    "                              multithread = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d97e7d-680d-45ea-bd47-9f391747011d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Code Breakdown</b>\n",
    "<br>\n",
    "\n",
    "- `filterAndTrim()` - primary function\n",
    "    - `fwd = `      - where we provide the object holding all the forward read input files\n",
    "    - `rev = `      - where we provide the object holding all the reverse read input files\n",
    "    - `filt = `     - where we provide the object holding what will be the output forward read files\n",
    "    - `filt.rev = ` - where we provide the object holding what will be the output reverse read files\n",
    "    - `trimLeft = ` - how many bases we want to have trimmed off the left side of the reads (providing them as a vector like this with two numbers means the first will used for the forward reads and the second for the reverse reads)\n",
    "    - `maxEE = `    - maximum \"expected error\" to allow for the forward and reverse reads (similar to above; you can read more about \"expected error\" [here](https://www.drive5.com/usearch/manual/exp_errs.html) and in its original publication [here](https://academic.oup.com/bioinformatics/article/31/21/3476/194979))\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9359c0fc-9232-49bb-80ce-6286c991a32f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "And we can check our files are where we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b054c8-3e54-4fb8-80f1-4e140c6fbd54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list.files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ac8a64-e4ae-4332-817d-2cb6f11c3145",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list.files(trimmed_and_filtered_reads_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c125680-7529-410c-959f-f7fa55a2e1f9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "**Now let's switch back to the [Setup and QC notebook](04-setup-QC.ipynb#Quality-assessment-of-filtered-reads) to run fastqc and multiqc on these files.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661dcfc4-30d2-49fa-821d-621e1c31fe18",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "### Generate error model of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b6c43d-3acc-4161-9237-5573356efc24",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Now we are going to generate error models by learning the specific error-signatures of our dataset. Each sequencing run, even when all goes well, will have its own subtle variations to its error profile. dada2 tries to learn and incorporate this information when it later tries to infer the true, starting biological sequences. Here we are running the function that does this on both the forward and reverse reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efd064e-504c-4b2a-96ff-d7ea1e1695d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "err_forward_reads <- learnErrors(fls = forward_filtered_files, multithread = 6)\n",
    "err_reverse_reads <- learnErrors(fls = reverse_filtered_files, multithread = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e324b0f-c3d2-4371-b79e-048cb9543812",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Code Breakdown</b>\n",
    "<br>\n",
    "\n",
    "- `learnErrors()` - primary function\n",
    "    - `fls = `          - where we provide the object holding all the input read files\n",
    "    - `multithread = `  - where we can specify how many jobs to run in parallel\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14759e61-444d-4661-b1ed-537f5d6bd121",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "### Inferring sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eb915a-356f-4fa6-9e3c-e5f23fb7139a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Here’s where dada2 gets to do what it was born to do, that is to do its best to infer true biological sequences. It does this by incorporating the error models it generated above, quality information for the reads, and abundances of each unique sequence, and then figuring out if each sequence is more likely to be of biological origin or more likely to have been introduced by a sequencing error. You can read more about the details of this in the [dada2 paper](https://www.nature.com/articles/nmeth.3869#methods) of course or looking through their [site](https://benjjneb.github.io/dada2/index.html).\n",
    "\n",
    "This step can be run on individual samples, which is the least computationally intensive manner, or on all samples together, which increases the function’s ability to resolve low-abundance ASVs. Imagine Sample A has 10,000 copies of sequence Z, and Sample B has 1 copy of sequence Z. Sequence Z would likely be filtered out of Sample B even though it was a “true” singleton among perhaps thousands of spurious singletons we needed to remove. Because running all samples together on large datasets can become impractical computationally, the developers also added a way to try to combine the best of both worlds that they refer to as pseudo-pooling, which is explained very nicely [on this page](https://benjjneb.github.io/dada2/pseudo.html#Pseudo-pooling). We will be using that method here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747df00c-4e3d-4cd8-bdad-5a95de695299",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forward_seqs <- dada(derep = forward_filtered_files, err = err_forward_reads, pool = \"pseudo\", multithread = 6)\n",
    "reverse_seqs <- dada(derep = reverse_filtered_files, err = err_reverse_reads, pool = \"pseudo\", multithread = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a001e1-a681-487d-9710-ccb2414a3402",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Code Breakdown</b>\n",
    "<br>\n",
    "\n",
    "- `dada()` - primary function\n",
    "    - `derep = ` - where we provide the object holding all the input read files\n",
    "    - `err = `   - where we provide the object created by the `learnErrors()` function we ran above\n",
    "    - `pool = `  - where we tell it the method to use (if any) to try to pool information across samples, as explained on [this page](https://benjjneb.github.io/dada2/pseudo.html#Pseudo-pooling)\n",
    "    - `multithread = `  - where we can specify how many jobs to run in parallel\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334c0550-ecc6-454c-8781-962275268ef3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "### Merging forward and reverse reads\n",
    "\n",
    "Now dada2 merges the forward and reverse ASVs to reconstruct our full target amplicons, requiring the overlapping region to be identical between the two reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c664a3c1-1c2b-4257-96f9-17b3aef11504",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_amplicons <- mergePairs(dadaF = forward_seqs, derepF = forward_filtered_files, \n",
    "                               dadaR = reverse_seqs, derepR = reverse_filtered_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f73ec2e-35f9-46d0-95bf-62978fe9bc09",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Code Breakdown</b>\n",
    "<br>\n",
    "\n",
    "- `mergePairs()` - primary function\n",
    "    - `dadaF = ` - where we provide the forward read object from the dada() function we ran above\n",
    "    - `derepF = ` - where we provide the object holding all the input forward read files\n",
    "    - `dadaR = ` - where we provide the reverse read object from the dada() function we ran above\n",
    "    - `derepR = ` - where we provide the object holding all the input reverse read files\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f93c4d-d48d-47a0-9645-119f4ccde3eb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "### Generating sequence table with counts per sample\n",
    "\n",
    "Now we can generate a count table with the `makeSequenceTable()` function. This is one of the main outputs from processing an amplicon dataset. It is also often referred to as a biome table, or an OTU matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cdfee5-4c0c-485e-99fe-883a052eed70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seqtab <- makeSequenceTable(merged_amplicons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbdda29-dac7-4467-9b48-10e31723e338",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "This isn't very friendly to look at yet, because it uses the full sequences as column names, but we'll make a more traditional one where we change that in a few steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459dabe0-dad6-4153-b7fc-4b5d27e35f2a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "### Removing putative chimeras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c91870-d27b-412d-b1d9-b407e4a68024",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Chimeras are technical artifacts made during PCR where different sequences merge together to form a new sequence, and this problem is extremely common during the generation of amplicon data. dada2 identifies likely chimeras by aligning each sequence with those that were recovered in greater abundance and then seeing if there are any lower-abundance sequences that can be made exactly by mixing left and right portions of two of the more-abundant ones. If so, these likely chimeric sequences are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81d8c70-cccb-46e9-ae6c-2cf875bd7e60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seqtab.nochim <- removeBimeraDenovo(unqs = seqtab, multithread = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2074a67-242d-445a-92c4-57b6df595578",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Code Breakdown</b>\n",
    "<br>\n",
    "\n",
    "- `removeBimeraDenovo()` - primary function\n",
    "    - `unqs = ` - where we provide the object we created with the `makeSequenceTable()` function above\n",
    "    - `multithread = `  - where we can specify how many jobs to run in parallel\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940a70e6-79ba-4a85-980c-37445f87d59f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "We can see how many unique sequences we had prior to chimera removal by looking at the number of columns in the object we made above with the `makeSequenceTable()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a16016c-9c99-4072-9630-a08d77fc60f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ncol(seqtab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffd0d36-cfcf-444d-ae60-b6308e71c141",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "And how many we had after removing likely chimeras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c4ead2-16a7-47a4-b308-fdc06dfd5ad8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ncol(seqtab.nochim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d8a1f6-5c5a-4992-bc2b-3ee87a5119ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ncol(seqtab.nochim) / ncol(seqtab) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298a117e-ece7-4a27-a8bc-fc3fdfc3bf04",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "That says we dropped quite a bit in terms of number of unique sequences, and we're only retaining ~17% of the total unique sequences recovered. But this is not the same as the number of actual fragments sequenced, because many of them are seen more than once. Here's a way we can look at that value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16e067c-d711-40a8-af8a-ee02e5ab51ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sum(seqtab.nochim) / sum(seqtab) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a7bc4e-7c69-4b9c-8e2d-508c0905f82b",
   "metadata": {},
   "source": [
    "Which tells us we retained ~96% of the initial sequences. This is a very common scenario with amplicon data, having many chimeric unique sequences recovered, but only making up a small portion of the total data sequenced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c91587-f162-4512-916e-ee64b93888ab",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "### Generating an overview of counts throughout processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6277ec2b-7e30-4dbf-84ab-621c0eb12150",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "It can be helpful to have a count of how many reads we had at each step along the way in one table. This can aid in finding any potentially problematic steps. The developers’ [DADA2 tutorial](https://benjjneb.github.io/dada2/tutorial.html) provides an example of a nice, quick way to pull out how many reads were dropped at various points of the pipeline. Here’s a slightly modified version adding in a final column of percent of reads retained from the start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a97d00-d39b-4ae1-b492-22aa8c9b5203",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# making a helper function\n",
    "getN <- function(x) sum(getUniques(x))\n",
    "\n",
    "summary_tab <- data.frame(row.names = sample_names,\n",
    "                          starting_read_pairs = filtered_out[, 1],\n",
    "                          filtered_read_pairs = filtered_out[, 2],\n",
    "                          fwd_ASVs = sapply(forward_seqs, getN),\n",
    "                          rev_ASVs = sapply(reverse_seqs, getN),\n",
    "                          merged_ASVs = sapply(merged_amplicons, getN),\n",
    "                          non_chimeras = rowSums(seqtab.nochim),\n",
    "                          final_perc_reads_retained = round(rowSums(seqtab.nochim) / filtered_out[, 1] * 100, 1)\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df4937b-7b77-4a29-8499-d25c8c51807e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "This is a very busy code block and not that straightforward for where we are at. So we aren't going to break every component down on this one, but in the future, running each part piece-by-piece and looking at what each is doing would be good practice if wanting to understand it better.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c654284-b501-4636-a344-57939fbf863b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "And here is what our summary table looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7c51a3-59cb-4443-be18-2a5c90c2fcfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5667ee-5014-4ecd-b657-9e8dba277d79",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Showing we retained about 80% of our starting reads, with most being dropped at the initial filtering step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43a107f-ccd1-47ba-8f17-829e85710a9f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "### Assigning taxonomy\n",
    "To assign taxonomy, we are going to use the [DECIPHER package](https://bioconductor.org/packages/release/bioc/html/DECIPHER.html). There are some DECIPHER-formatted databases available [here](http://www2.decipher.codes/Classification/TrainingSets/), which is where the one that we use below comes from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c8ccdd-6cad-4ff5-939d-caecc935a513",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loading library used for taxonomy assignment\n",
    "library(DECIPHER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ca57c4-0495-4495-9d5a-1cabb2c0b69c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creating the type of object needed\n",
    "dna <- DNAStringSet(getSequences(seqtab.nochim))\n",
    "    # this is pulling the sequences out of our seqtab.nochim object with the getSequences() function, \n",
    "    # and passing them to the DNAStringSet() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291bdb9e-b516-4562-9a20-9c51cfcda63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading reference\n",
    "download.file(url = \"http://www2.decipher.codes/Classification/TrainingSets/SILVA_SSU_r138_2019.RData\", destfile = \"SILVA_SSU_r138_2019.RData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1b6052-a6e1-4fed-9675-817e96543334",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loading reference into R objects\n",
    "load(\"SILVA_SSU_r138_2019.RData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4189374-c862-4463-9619-217c00b8deda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# took about 60 seconds with subset dataset on local with 4 cpus\n",
    "# classifying sequences\n",
    "tax_info <- IdTaxa(test = dna, trainingSet = trainingSet, strand = \"both\", processors = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec38eca2-0cc5-42f7-a123-a61286d81cad",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Code Breakdown</b>\n",
    "<br>\n",
    "\n",
    "- `IdTaxa()` - primary function\n",
    "    - `test = ` - where we provide the dna object holding our sequences we want to classify\n",
    "    - `trainingSet = ` - where we provide the object holding the reference information (it was loaded as \"trainingSet\" by the above `load()` function)\n",
    "    - `strand = ` - specifying to check both forward and reverse strands\n",
    "    - `processors = ` - where we can specify how many jobs to run in parallel\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e079e9d-78b9-42c0-b645-37b775883c59",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "And we can peek at this object holding our classifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c8225f-335c-4556-a81f-725045815c66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tax_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16375936-e2a0-49d3-be8f-da9955ea8892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and removing the reference file as we don't need it anymore\n",
    "unlink(\"SILVA_SSU_r138_2019.RData\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46a061f-fa00-41f4-947d-933a915402cb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "### Generating and writing standard outputs\n",
    "\n",
    "The typical standard outputs form amplicon processing are: 1) a fasta file of our unique ASVs; 2) a count table showing how many times each unique ASV was detected in each sample; and 3) a taxonomy table linking our ASV IDs to their assigned taxonomy. Here is one way we can generate those files from our dada2 objects in R.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "This code can get a little busy too, and it's a little beyond our current scope to dig into it all. So like above, we aren't going to break every component down, but running each part piece-by-piece and looking at what it's doing would be good practice in the future if wanting to understand it better.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928eddfa-a13e-4ca4-812d-1821d9bdddca",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "**1. Making and writing out a fasta file of our recovered ASV sequences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf219e0-cb12-4eb3-a86b-0e183f328474",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# giving sequences more manageable names\n",
    "asv_seqs <- colnames(seqtab.nochim)\n",
    "asv_headers <- vector(dim(seqtab.nochim)[2], mode = \"character\")\n",
    "\n",
    "for (i in 1:dim(seqtab.nochim)[2]) {\n",
    "    asv_headers[i] <- paste(\">ASV\", i, sep = \"_\")\n",
    "}\n",
    "\n",
    "# making then writing out a fasta of final ASV sequences\n",
    "asv_fasta <- c(rbind(asv_headers, asv_seqs))\n",
    "write(asv_fasta, paste0(final_outputs_dir, \"/ASVs.fasta\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab504c15-cc5d-411a-a712-2b348805e161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can look at the fasta object we made with the head() function\n",
    "head(asv_fasta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97965458-8052-4bfb-9773-02526d430613",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "**2. Making and writing out a count table of how many times each ASV was detected in each sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c26a741-1226-4a89-9878-b47bb13592f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# making and writing out a count table\n",
    "asv_starting_tab <- t(seqtab.nochim)\n",
    "colnames(asv_starting_tab) <- sample_names\n",
    "\n",
    "asv_ids <- sub(\">\", \"\", asv_headers)\n",
    "\n",
    "count_tab <- data.frame(\"ASV_ID\" = asv_ids, asv_starting_tab, check.names = FALSE, row.names = NULL)\n",
    "\n",
    "write.table(count_tab, paste0(final_outputs_dir, \"/ASV_counts.tsv\"), sep = \"\\t\", quote = FALSE, row.names = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64db96e-a2e1-4573-8933-449b3cb8497b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we can peek at this table with the head() function\n",
    "head(count_tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13494b7b-fd96-4760-83b0-a94fcb444f09",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "**3. Making and writing out a table of taxonomy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eed6081-dc3b-4dcc-b8a9-d7b95e7448b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# making and writing out a table of taxonomy, with any unclassified as \"NA\"\n",
    "ranks <- c(\"domain\", \"phylum\", \"class\", \"order\", \"family\", \"genus\", \"species\")\n",
    "\n",
    "starting_tax_tab <- t(sapply(tax_info, function(x) {\n",
    "    m <- match(ranks, x$rank)\n",
    "    taxa <- x$taxon[m]\n",
    "    taxa[startsWith(taxa, \"unclassified_\")] <- NA\n",
    "    taxa\n",
    "}))\n",
    "\n",
    "colnames(starting_tax_tab) <- ranks\n",
    "tax_tab <- data.frame(\"ASV_ID\" = asv_ids, starting_tax_tab, row.names = NULL)\n",
    "\n",
    "write.table(tax_tab, paste0(final_outputs_dir, \"/ASV_taxonomy.tsv\"), sep = \"\\t\", quote = FALSE, row.names = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b205fdb-9ab6-4e82-9a10-6362ea3b292e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we can also peek at this table\n",
    "head(tax_tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f8f5b0-2497-47d8-a086-f295ca053b2f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbe3aaa-210a-4f44-b7de-c2d35845d5df",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "And that's it for baseline processing now that we have our standard goods. **Next we'll move onto the [analysis notebook](06-amplicon-analysis.ipynb).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7941e3f2-7ff9-4b9d-b2be-aaf5e659bdd4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "\n",
    "---\n",
    "---\n",
    "\n",
    "[**Previous:** 4. Setup and QC](04-setup-QC.ipynb)\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: right\"><a href=\"06-amplicon-analysis.ipynb\"><b>Next:</b> 6. Amplicon analysis</a></div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
